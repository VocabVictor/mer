#!/usr/bin/env python3
"""
Mamba æœ€ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def run_simple_demo(device):
    """è¿è¡Œç®€åŒ–ç‰ˆ Mamba æ¦‚å¿µæ¼”ç¤º"""
    print("\nğŸ“ Mambaæ¦‚å¿µæ¼”ç¤º (ç®€åŒ–ç‰ˆ):")
    
    class SimpleMamba(nn.Module):
        def __init__(self, d_model):
            super().__init__()
            self.d_model = d_model
            self.linear1 = nn.Linear(d_model, d_model * 2)
            self.conv1d = nn.Conv1d(d_model, d_model, 3, padding=1, groups=d_model)
            self.linear2 = nn.Linear(d_model, d_model)
            
        def forward(self, x):
            # x: (batch, seq_len, d_model)
            residual = x
            x = self.linear1(x)  # æ‰©å±•
            x1, x2 = x.chunk(2, dim=-1)  # åˆ†å‰²
            
            # ç®€åŒ–çš„SSMæ“ä½œ (è¿™é‡Œç”¨å·ç§¯æ¥æ¨¡æ‹Ÿ)
            x1_conv = self.conv1d(x1.transpose(1, 2)).transpose(1, 2)
            x1 = F.silu(x1_conv)  # ä½¿ç”¨ F.silu è€Œä¸æ˜¯ torch.silu
            
            # é—¨æ§æœºåˆ¶
            x = x1 * torch.sigmoid(x2)
            
            # è¾“å‡ºæŠ•å½±
            x = self.linear2(x)
            return x + residual
    
    # æµ‹è¯•ç®€åŒ–ç‰ˆæœ¬
    model = SimpleMamba(d_model=256).to(device)
    x = torch.randn(2, 64, 256).to(device)
    
    with torch.no_grad():
        output = model(x)
    
    print(f"ç®€åŒ–ç‰ˆè¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"ç®€åŒ–ç‰ˆè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("âœ… æ¦‚å¿µæ¼”ç¤ºå®Œæˆï¼")

def main():
    print("ğŸ Mamba æœ€ç®€å•ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    print("\nğŸ“¦ å®‰è£…è¯´æ˜:")
    print("pip install causal-conv1d>=1.4.0")
    print("pip install mamba-ssm")
    print("æˆ–è€…: pip install 'mamba-ssm[causal-conv1d]'")
    
    print("\nğŸ”§ ç¤ºä¾‹1: åŸºæœ¬ Mamba å—")
    try:
        from mamba_ssm import Mamba
        
        # é…ç½®å‚æ•°
        batch_size = 2
        seq_length = 64
        d_model = 256
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        x = torch.randn(batch_size, seq_length, d_model).to(device)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # åˆ›å»º Mamba æ¨¡å‹
        model = Mamba(
            d_model=d_model,    # æ¨¡å‹ç»´åº¦
            d_state=16,         # SSM çŠ¶æ€æ‰©å±•å› å­
            d_conv=4,           # æœ¬åœ°å·ç§¯å®½åº¦
            expand=2,           # å—æ‰©å±•å› å­
        ).to(device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(x)
        
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print("âœ… åŸºæœ¬ Mamba å—è¿è¡ŒæˆåŠŸï¼")
        
    except ImportError as e:
        print("âŒ æœªå®‰è£… mamba-ssmï¼Œè¯·å…ˆå®‰è£…:")
        print("pip install mamba-ssm")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        
        # è¿è¡Œç®€åŒ–ç‰ˆæœ¬æ¥æ¼”ç¤ºæ¦‚å¿µ
        run_simple_demo(device)
    
    except Exception as e:
        print(f"âŒ mamba-ssm è¿è¡Œå‡ºé”™: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äº CUDA ç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´çš„")
        print("å»ºè®®å°è¯•é‡æ–°ç¼–è¯‘å®‰è£… mamba-ssm:")
        print("pip uninstall mamba-ssm causal-conv1d")
        print("pip install causal-conv1d>=1.4.0 --no-cache-dir")
        print("pip install mamba-ssm --no-cache-dir")
        
        # è¿è¡Œç®€åŒ–ç‰ˆæœ¬
        run_simple_demo(device)
    
    print("\nğŸš€ ç¤ºä¾‹2: é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨æ¡†æ¶")
    print("# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„ä»£ç ç¤ºä¾‹:")
    print("from transformers import AutoTokenizer")
    print("from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel")
    print("")
    print("# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
    print("tokenizer = AutoTokenizer.from_pretrained('state-spaces/mamba-130m')")
    print("model = MambaLMHeadModel.from_pretrained('state-spaces/mamba-130m')")
    print("")
    print("# æ¨ç†")
    print("text = 'Hello, how are you?'")
    print("tokens = tokenizer.encode(text, return_tensors='pt')")
    print("with torch.no_grad():")
    print("    output = model(tokens)")
    print("    logits = output.logits")
    
    print("\nğŸ“š å­¦ä¹ èµ„æº:")
    print("- å®˜æ–¹ä»“åº“: https://github.com/state-spaces/mamba")
    print("- è®ºæ–‡: https://arxiv.org/abs/2312.00752")
    print("- HuggingFaceæ¨¡å‹: https://huggingface.co/state-spaces")
    
    print("\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    main() 